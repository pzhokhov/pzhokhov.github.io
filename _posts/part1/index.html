<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">ol{margin:0;padding:0}table td,table th{padding:0}.c6{border-right-style:solid;padding:8pt 8pt 8pt 8pt;border-bottom-color:#dddddd;border-top-width:1pt;border-right-width:1pt;border-left-color:#dddddd;vertical-align:top;border-right-color:#dddddd;border-left-width:1pt;border-top-style:solid;background-color:#ffffff;border-left-style:solid;border-bottom-width:1pt;width:468pt;border-top-color:#dddddd;border-bottom-style:solid}.c0{color:#333333;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10.5pt;font-family:"Arial";font-style:normal}.c2{color:#333333;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:15pt;font-family:"Arial";font-style:normal}.c16{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:20pt;font-family:"Arial";font-style:normal}.c8{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c20{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:26pt;font-family:"Arial";font-style:normal}.c4{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left;height:11pt}.c1{padding-top:8pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c5{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c9{text-decoration-skip-ink:none;font-size:10.5pt;-webkit-text-decoration-skip:none;color:#1155cc;text-decoration:underline}.c21{padding-top:20pt;padding-bottom:6pt;line-height:1.15;page-break-after:avoid;text-align:left}.c13{padding-top:0pt;padding-bottom:3pt;line-height:1.15;page-break-after:avoid;text-align:left}.c14{padding-top:23pt;padding-bottom:0pt;line-height:1.5;text-align:left}.c7{border-spacing:0;border-collapse:collapse;margin-right:auto}.c18{text-decoration-skip-ink:none;-webkit-text-decoration-skip:none;color:#1155cc;text-decoration:underline}.c19{background-color:#ffffff;max-width:468pt;padding:72pt 72pt 72pt 72pt}.c3{color:#333333;font-size:10.5pt}.c10{color:inherit;text-decoration:inherit}.c12{height:11pt}.c11{height:172pt}.c17{height:16pt}.c15{height:28pt}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c19"><p class="c13 title" id="h.f13sjywubl5q"><span class="c20">Zero-overhead scalable machine learning. </span></p><p class="c4"><span class="c8"></span></p><p class="c5"><span>We analyze the complexity overhead and a learning curve associated with the transition from quick-and-dirty machine learning experiments to large-scale production-grade models with the recently released </span><span class="c18"><a class="c10" href="https://www.google.com/url?q=https://aws.amazon.com/sagemaker/&amp;sa=D&amp;ust=1513886318984000&amp;usg=AFQjCNEnK0xFyoXW_bSON7Y823rWKAKd-g">Amazon SageMaker</a></span><span>&nbsp;and open-source project </span><span class="c18"><a class="c10" href="https://www.google.com/url?q=http://studio.ml&amp;sa=D&amp;ust=1513886318985000&amp;usg=AFQjCNHu3IWddLXzqAeVoejqzPv1z-6U7g">Studio.ML</a></span></p><p class="c4"><span class="c8"></span></p><p class="c5"><span class="c3">Virtually every domain of human expertise is facing a rapid increase in the integration of machine learning solutions and tools. With the number of machine learning experiments and models growing, data science teams in big and small companies realize the need for a unifying framework, one that lets data scientists build on top of their own models and experiments as well as leverage the efforts of other community teams and members in an efficient manner. The &quot;data science github&quot; concept, however, faces multiple challenges (mainly related to the usage of large datasets, large amounts of computing resources and custom hardware). Multiple attempts to solve these issues have been made, the most well-known are </span><span class="c9"><a class="c10" href="https://www.google.com/url?q=https://cloud.google.com/ml-engine/&amp;sa=D&amp;ust=1513886318985000&amp;usg=AFQjCNE89D7C9OjaH7lUEu0BlCFhBbgLHw">Google Cloud ML</a></span><span class="c0">&nbsp;and the recently released Amazon SageMaker. </span></p><p class="c4"><span class="c0"></span></p><p class="c5"><span class="c0">In this blog series, we&rsquo;ll build several models ranging from very simple toy examples to state-of-the-art deep neural networks presented at NIPS 2017. We&rsquo;ll make the model training reproducible in the cloud using modern frameworks, and show why Sentient Technologies continues to support the open-source project Studio.ML. </span></p><p class="c4"><span class="c0"></span></p><h1 class="c21" id="h.umoi0hs2cdfi"><span class="c16">Part 1. K-means with Amazon SageMaker and Studio.ML.</span></h1><p class="c1"><span class="c0">The story revolves around a fairly simple exercise (chosen from the SageMaker getting started guide to ensure my lack of knowledge with the SageMaker is not affecting the results) - building a K-means model of MNIST data. </span></p><p class="c1"><span class="c0">For &nbsp;people not familiar with K-means - it is an unsupervised learning algorithm (i.e. we&#39;ll feed the algorithm the images of digits from MNIST, but not the labels) that searches for centers of clusters. Each step consists of assigning data samples (in our case, 28x28 grayscale MNIST images -&gt; 784 dimensional vector samples) to clusters, and then moving the cluster centers to the averaged coordinates of data samples in each cluster. At the prediction time, we find the cluster center closest to the input image. </span></p><h2 class="c14" id="h.8ctjpxs5den"><span class="c2">K-Means with SageMaker</span></h2><p class="c1"><span class="c0">To get started with SageMaker, one needs to create a notebook instance. SageMaker comes with a very user-friendly UI that walks you through the process.</span></p><p class="c1"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 354.67px;"><img alt="" src="images/image11.png" style="width: 624.00px; height: 354.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1"><span class="c0">Notebook comes with an AMI (instance image) that has all common ML libraries pre-installed. </span></p><p class="c1"><span class="c0">Once the notebook creation request is submitted, AWS provisions and sets up the instance; this process &nbsp;usually takes 4-6 minutes. When the notebook is created, one clicks &quot;Open&quot; in the SageMaker console to open a familiar jupyter notebook window. </span></p><p class="c1"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 583.50px; height: 369.91px;"><img alt="" src="images/image16.png" style="width: 583.50px; height: 369.91px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1"><span class="c0">The first cell deals with imports and downloading the MNIST data, while the second cell converts and uploads the data to the s3 bucket (the SageMaker training routine assumes data resides within S3). </span></p><p class="c1"><span class="c0">Now we are ready to launch training: </span></p><p class="c1"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 389.33px;"><img alt="" src="images/image10.png" style="width: 624.00px; height: 389.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1"><span class="c0">Okay, so now we have a trained model. How can we use / validate the model? The model artifacts do not seem to be easy to introspect; however, we can serve and predict using the served model: </span></p><p class="c1"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 173.33px;"><img alt="" src="images/image12.png" style="width: 624.00px; height: 173.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1"><span class="c0">Note that deploying the model takes a little while (in this case, 12 minutes - longer than the training itself) which is ok if the deployed model will be used in production, but sounds excessive for simple model validation. </span></p><p class="c1"><span class="c0">After predicting, we can visualize the images from various clusters. Learning is unsupervised, so the digits in the clusters do not have to correspond to the cluster number. We see that images that look like 6 are assigned to cluster number 0. </span></p><p class="c1"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 308.00px;"><img alt="" src="images/image3.png" style="width: 624.00px; height: 308.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1 c12"><span class="c0"></span></p><p class="c1"><span class="c0">Another limitation of the prediction via a call to an endpoint is the maximum number of images one can process at a time:</span></p><p class="c1"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 326.67px;"><img alt="" src="images/image9.png" style="width: 624.00px; height: 326.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><h2 class="c14 c17" id="h.v8u6q8ugw0zs"><span class="c2"></span></h2><h2 class="c14" id="h.n9t6hfntmrtj"><span class="c2">K-Means with Jupyter Notebook</span></h2><p class="c1"><span class="c0">Let&#39;s look into how would we run the same process without SageMaker. We&#39;ll need python packages urllib (to download the data), sklearn (to actually perform the k-means), matplotlib (for visualization) and jupyter (for the notebook). If you have done any machine learning in python before, chances are you have all of those already. Otherwise, they can be installed via entering</span></p><a id="t.0e6e887fecc856ae68f4bd2f447dd6ce36abd087"></a><a id="t.0"></a><table class="c7"><tbody><tr class="c15"><td class="c6" colspan="1" rowspan="1"><p class="c1"><span class="c0">pip install urllib sklearn matplotlib jupyter</span></p></td></tr></tbody></table><p class="c1"><span class="c0">in the command line. &nbsp;Then the training is achieved with the following cells</span></p><p class="c1"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 356.00px;"><img alt="" src="images/image4.png" style="width: 624.00px; height: 356.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1 c12"><span class="c0"></span></p><p class="c1"><span class="c0">The prediction is done as follows (this time images that look like 6 got assigned to cluster 1)</span></p><p class="c1"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 342.67px;"><img alt="" src="images/image15.png" style="width: 624.00px; height: 342.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1"><span class="c0">Note that when using sklearn, model introspection is very simple. For example, the following code will show the images that correspond to the cluster centroids. The cluster centroids are more blurry than the actual digit images because they are the means of all images in the cluster. </span></p><p class="c1"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 138.67px;"><img alt="" src="images/image1.png" style="width: 624.00px; height: 138.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1 c12"><span class="c0"></span></p><p class="c1"><span class="c3">So far, we can see that for simple tasks, using jupyter notebook with the right package (in this case, sklearn) can be much simpler and more flexible than using the SageMaker. What about complicated tasks? If the complexity is in the amount of data, there is no doubt that SageMaker will shine - after all, the careful reader noticed that we were using c4.8xlarge instance for training, which can crunch numbers much faster than my laptop. However, what if the complexity is also in a new algorithm (either training algorithm, or model wrapper, or both)? In SageMaker you need to build your own container (either the same for training and inference, or two different ones) following specific guidelines, then register and use the container (</span><span class="c9"><a class="c10" href="https://www.google.com/url?q=https://github.com/awslabs/amazon-sagemaker-examples/blob/master/advanced_functionality/scikit_bring_your_own/scikit_bring_your_own.ipynb&amp;sa=D&amp;ust=1513886318993000&amp;usg=AFQjCNEPzlJ5QETxIG_TJl5kcRIYqIESBw">https://github.com/awslabs/amazon-sagemaker-examples/blob/master/advanced_functionality/scikit_bring_your_own/scikit_bring_your_own.ipynb</a></span><span class="c3">). The existing primitives help you along the way; but it is still painful especially when making containers to be gpu-compatible (</span><span class="c9"><a class="c10" href="https://www.google.com/url?q=http://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-training-algo.html&amp;sa=D&amp;ust=1513886318994000&amp;usg=AFQjCNHEnJwFt3BE2wuoPaNEU9Rnn1-0gg">http://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-training-algo.html</a></span><span class="c0">). The biggest pain point for me though would be debugging the training algorithm and navigating the way it is wired up within the container. By contrast, in the regular jupyter notebook, as soon as something goes wrong, you can drop into pdb, inspect variables and see immediately what is going on. We have already seen a machine learning analog of debugging - model introspection - being easily done in jupyter notebook, but being fairly hard (if not impossible) in SageMaker. </span></p><p class="c1"><span class="c0">But wait you might say, how is it fair to compare plain jupyter with SageMaker? Anyone can use a screwdriver, but operating a CNC milling machine requires training. Besides, surely the fact that one can carry a screwdriver in a pocket does not make the screwdriver more useful than the milling machine? What about serving the models? What about training on custom hardware? What about hyperparameter optimization? What about the model provenance? </span></p><p class="c1"><span class="c0">And finally, why is Studio.ML in the title of this blog? </span></p><p class="c1"><span class="c0">The answer to all of this questions is the following. Studio.ML gives you all of the above (serving, custom hardware in the cloud, hyperparameter optimization, model provenance) without leaving the comfort zone of the locally (or where ever your preference is)-running jupyter notebook or python command line. In a sense, it is a CNC milling bit for your pocket data science screwdriver. </span></p><h2 class="c14" id="h.x36m4y9dstpt"><span class="c2">K-Means with Studio.ML</span></h2><p class="c1"><span class="c0">Let us install studioml package via</span></p><a id="t.3e505f8202c7c07c70387b0b7deba3a080c5e974"></a><a id="t.1"></a><table class="c7"><tbody><tr class="c15"><td class="c6" colspan="1" rowspan="1"><p class="c1"><span class="c0">pip install studioml</span></p></td></tr></tbody></table><p class="c1 c12"><span class="c0"></span></p><p class="c1"><span class="c0">Then, we use the same jupyter notebook as in the last exercise (K-means with sklearn), and add a single line to the imports section that imports cell magics from studio:</span></p><p class="c1"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 73.33px;"><img alt="" src="images/image5.png" style="width: 624.00px; height: 73.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1"><span class="c0">To start an experiment with studio, we simply add a cell magic %%studio_run to the notebook cell:</span></p><p class="c1"><span class="c0">Technically, studio.ml returns all the variables created in the cell, so I also erase train_set and test_set variables to prevent them being returned)</span></p><p class="c1"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 206.67px;"><img alt="" src="images/image8.png" style="width: 624.00px; height: 206.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1"><span class="c0">The link in the beginning of the experiment sends us to a central repo of experiments, to the experiment page that shows experiment progress, artifacts, and list of python packages necessary to reproduce the experiment.</span></p><p class="c1"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 542.67px;"><img alt="" src="images/image14.png" style="width: 624.00px; height: 542.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1"><span class="c3">The code in the cell runs in 6 minutes (slightly longer than plain sklearn due to, mainly, compressing, storing in the cloud, and returning the validation data; </span><span class="c3">still 3 minutes faster</span><span class="c0">&nbsp;than using a dedicated training instance in SageMaker)</span></p><p class="c1"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 330.67px;"><img alt="" src="images/image7.png" style="width: 624.00px; height: 330.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1"><span class="c0">If the training is more heavy-weight and actually requires extended usage of powerful instances, we can request them from the cloud by modifying studio_run magic. For example, </span></p><a id="t.51ba2bc206ba6e95c7d7c86488d802a838c8bf0b"></a><a id="t.2"></a><table class="c7"><tbody><tr class="c15"><td class="c6" colspan="1" rowspan="1"><p class="c1"><span class="c0">%%studio_run --cloud=ec2spot --cpus=16</span></p></td></tr></tbody></table><p class="c1"><span class="c0">will run the training on a spot instance with 16 cpus (the c4.4xlarge instance will be selected for that). &nbsp;The cool part is that once the training is complete, the rest of the notebook code is exactly the same as it used to be, including prediction and displaying cluster centers.</span></p><p class="c1"><span class="c0">This time 6-like digits accidentally ended up in a cluster with a correct number</span></p><p class="c1"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 492.00px;"><img alt="" src="images/image2.png" style="width: 624.00px; height: 492.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1 c12"><span class="c0"></span></p><p class="c1"><span class="c0">So far, we have shown that Studio.ML provides zero-overhead experiment provenance and provisioning of custom hardware, including spot instances, different clouds (for now Studio.ML has support for Amazon EC2 and Google Cloud). What about model serving? Studio.ML does come with model serving primitives for Keras, but not for sklearn (not yet, at least); however, the process of making one is very transparent. </span></p><p class="c1"><span class="c0">Let us write the following python script to be used as a model wrapper and call in kmeans_serve.py</span></p><a id="t.4872610b5e32017fcdd1e0a96f6728604b464995"></a><a id="t.3"></a><table class="c7"><tbody><tr class="c11"><td class="c6" colspan="1" rowspan="1"><p class="c1"><span class="c0">import pickle<br>import os<br><br><br>def create_model(modeldir):<br> &nbsp; &nbsp;with open(os.path.join(modeldir, &#39;kmeans.pkl.gz&#39;)) as f:<br> &nbsp; &nbsp; &nbsp; &nbsp;kmeans = pickle.loads(f.read())<br><br> &nbsp; &nbsp;def model_function(input_dict):<br> &nbsp; &nbsp; &nbsp; &nbsp;predictions = kmeans.predict(pickle.loads(input_dict[&#39;input_data&#39;]))<br> &nbsp; &nbsp; &nbsp; &nbsp;return {&quot;output_data&quot;: pickle.dumps(predictions)}<br><br> &nbsp; &nbsp;return model_function</span></p></td></tr></tbody></table><p class="c1"><span class="c0">which loads the pickled model kmeans from file kmeans.pkl in the directory modeldir (passed as an argument) and returns a function that, given a dictionary {&quot;input_data&quot;: &lt;pickled_input_data&gt;} performs inference and returns the results as a dictionary {&quot;output_data&quot;: &lt;pickled_output_data&gt;}. This approach can be used with any model that consumes pickleable data. </span></p><p class="c1"><span class="c0">We then run the following command:</span></p><a id="t.6c8e2baec31bfae0d1be35b4a63333ca2ef2b643"></a><a id="t.4"></a><table class="c7"><tbody><tr class="c15"><td class="c6" colspan="1" rowspan="1"><p class="c1"><span class="c0">studio serve 1513115524_jupyter_7afb38f0-9918-48b8-9921-0d29f44f421d --wrapper=kmeans_serve.py<br></span></p></td></tr></tbody></table><p class="c1"><span class="c0">where 1513115524_jupyter_7afb38f0-9918-48b8-9921-0d29f44f421d is the key of the experiment returned in the output of %%studio_run cell magic. </span></p><p class="c1"><span class="c0">This command will serve the model locally, so in our notebook the following command generates the predictions:</span></p><p class="c1"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 73.33px;"><img alt="" src="images/image6.png" style="width: 624.00px; height: 73.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1"><span class="c0">As before, the predictions can be visualized as follows: </span></p><p class="c1"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 245.33px;"><img alt="" src="images/image13.png" style="width: 624.00px; height: 245.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1"><span class="c0">Serving does support EC2 and GCloud, so, for example, a command </span></p><a id="t.f36d997da4f21eb78e1a0af3da2d4f68637ef3c2"></a><a id="t.5"></a><table class="c7"><tbody><tr class="c15"><td class="c6" colspan="1" rowspan="1"><p class="c1"><span class="c0">studio serve 1513115524_jupyter_7afb38f0-9918-48b8-9921-0d29f44f421d --wrapper=kmeans_serve.py --cloud=ec2</span></p></td></tr></tbody></table><p class="c1"><span class="c0">will serve the model on an ec2 instance. </span></p><p class="c1"><span class="c0">Served models have an automatic expiration time (by default, one hour) so one does not have to worry about runaway instances. </span></p><h2 class="c14" id="h.gamksppgm35l"><span class="c2">Summary</span></h2><p class="c1"><span class="c0">We compared the experience of building a simple k-means model on MNIST dataset in SageMaker, jupyter notebook, and studio.ml. SageMaker provides a rich toolset out of the box, however, those tools are somewhat out of line with the natural jupyter notebook way; extension of the SageMaker toolbox to meet custom needs seems fairly complicated. In contrast, Studio.ML can seamlessly extend jupyter notebooks to provide the experiment provenance, training and / or inference on custom cloud compute (including spot instances) and serving. </span></p><p class="c1"><span class="c3">In our next blog, we&rsquo;ll look into training the Fader Network (</span><span>(</span><span class="c18"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/pdf/1706.00409.pdf&amp;sa=D&amp;ust=1513886319004000&amp;usg=AFQjCNETOuHXdX3Oz3kNoPqmgohNCEbN2g">https://arxiv.org/pdf/1706.00409.pdf</a></span><span class="c3">) - a neural network that can interpolate and swap between the attributes of an image such as presence/absence of glasses, open/closed eyes etc. The Fader Networks require a lot of computational resources to train, and as such, they are a tempting and yet difficult to chew fruit for data scientists outside large corporations such as Google or Facebook, which makes them a good demo for the full power of machine learning provenance frameworks such as the SageMaker or Studio.ML. </span></p><p class="c4"><span class="c8"></span></p></body></html>